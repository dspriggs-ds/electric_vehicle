{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41060c60-7f22-4823-9e1d-aafc35cd871a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Washington State Electric Vehicle Data\n",
    "The original premise or the reason why for working with this data is to confirm or deny if Electric Vehicles are becoming more popular or not. Also, I decided, in an effort to keep my skills up, want to perform some exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7beeff-6bd5-44a1-8839-340181f7ef98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imports all functions from the pyspark.sql.functions module in PySpark. These functions are used for various operations on DataFrames, such as data manipulation, aggregation, and transformation. By importing everything with the * wildcard, you can use any function from this module directly in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282c4854-e0a8-4db6-b43c-d08ae10721a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e67a7dc-0c21-4bba-ac8d-64d5fb685dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This Python code uses PySpark to read a CSV file into a DataFrame.\n",
    "Here's a concise explanation:\n",
    "1. bronze_path is defined as the file path to the CSV file containing electric vehicle title and registration activity data.\n",
    "2. ev_df is created by reading the CSV file located at bronze_path using Spark's read method.\n",
    "3. The format(\"csv\") specifies that the file format is CSV.\n",
    "4. The option(\"header\", \"true\") indicates that the first row of the CSV file contains header information (column names).\n",
    "5. The load(bronze_path) method loads the CSV file into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d9d8b7-4ba2-4efe-b7bb-5dbd28fdc1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"/Volumes/main/djsprojects/evdata/bronze/Electric_Vehicle_Title_and_Registration_Activity.csv\"\n",
    "ev_df=spark.read.format(\"csv\")\\\n",
    "           .option(\"header\", \"true\")\\\n",
    "           .option(\"inferSchema\", \"true\")\\\n",
    "           .option(\"preferDate\", \"true\")\\\n",
    "           .load(bronze_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0752fa-0bfe-41b2-af9c-5fdbee7fee0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we are going to do some data cleansing to ensure the date is formatted in a manner which we use to analyze the data:\n",
    "\n",
    "1. Converts the \"Sale Date\" column from a string format to a date format using the to_date function, with the date format specified as \"MMMM dd yyyy\" (e.g., \"January 01 2020\").\n",
    "2. Converts the \"Transaction Date\" column from a string format to a date format using the to_date function, with the same date format \"MMMM dd yyyy\".\n",
    "\n",
    "The withColumn method is used to create or replace the columns with the newly formatted date values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c401df06-a3d5-4171-adf4-527b71ceaa78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ev_df = ev_df.withColumn(\"Sale Date\", to_date(col(\"Sale Date\"), \"MMMM dd yyyy\"))\n",
    "ev_df = ev_df.withColumn(\"Transaction Date\", to_date(col(\"Transaction Date\"), \"MMMM dd yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cecc284-9c10-4c4f-984a-f6b20350aa79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The printSchema() method is called on a DataFrame object named ev_df. This method prints out the schema of the DataFrame, which includes the column names, data types, and nullable information. This is useful for understanding the structure of the data contained in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6561fb2b-912c-45ce-9844-13d281550b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ev_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf772ebe-52ec-4427-80f2-c99814136ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This Python code uses PySpark to transform the date columns in a DataFrame ev_df. Specifically, it converts the \"Sale Date\" and \"Transaction Date\" columns from their string format (\"MM/dd/yyyy\") to date format.\n",
    "\n",
    "1. withColumn(\"Sale Date\", to_date(col(\"Sale Date\"), \"MM/dd/yyyy\")):\n",
    "\n",
    "  - Converts the values in the \"Sale Date\" column from a string format (e.g., \"12/31/2021\") to a date format.\n",
    "\n",
    "2. withColumn(\"Transaction Date\", to_date(col(\"Transaction Date\"), \"MM/dd/yyyy\")):\n",
    "\n",
    "-   Converts the values in the \"Transaction Date\" column from a string format to a date format.\n",
    "The to_date function is used here for the conversion, and it follows the specified date pattern \"MM/dd/yyyy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc78ffab-5294-4d04-aeed-bb7fec54e54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following code utilizes the built Databricks functionality provide a data profile of the dataframe ev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28ef77b-3754-4ac3-9ce8-b5ef80cd0cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(ev_df,precise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6471460-febb-4c07-9204-c0d354dbc6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This Python code processes a DataFrame containing electric vehicle (EV) transaction data and visualizes the number of transactions over time using a line plot. Here's a step-by-step explanation:\n",
    "\n",
    "1. Group and Count Transactions:\n",
    "\n",
    "    - The code groups the ev_df DataFrame by the \"Transaction Date\" column and counts the number of transactions for each date.\n",
    "   - The result is converted to a Pandas DataFrame named ev_count_df.\n",
    "2. Convert Date Column:\n",
    "\n",
    "    - The \"Transaction Date\" column in ev_count_df is converted to a datetime format.\n",
    "3. Extract Year and Month:\n",
    "\n",
    "    - Two new columns, \"Transaction_Year\" and \"Transaction_Month\", are created by extracting the year and month from the \"Transaction Date\" column, respectively.\n",
    "4. Plot the Data:\n",
    "\n",
    "    - A line plot is created using Seaborn (sns.lineplot), with the x-axis representing the transaction year and the y-axis representing the count of transactions.\n",
    "    - The plot is labeled with \"Year\" on the x-axis, \"Number of Transactions\" on the y-axis, and titled \"Electric Vehicle Transactions in Washington State\".\n",
    "5. Display the Plot:\n",
    "\n",
    "    - The plot is displayed using plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82dd019-4edf-4a9b-951e-b46e0e271b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ev_count_df = ev_df\\\n",
    "                  .groupby(\"Transaction Date\")\\\n",
    "                  .count()\\\n",
    "                  .toPandas()\n",
    "ev_count_df[\"Transaction Date\"] = ev_count_df[\"Transaction Date\"].astype('datetime64[ns]')\n",
    "ev_count_df[\"Transaction_Year\"]  = ev_count_df[\"Transaction Date\"].dt.year\n",
    "ev_count_df[\"Transaction_Month\"] = ev_count_df[\"Transaction Date\"].dt.month\n",
    "             \n",
    "\n",
    "ev_plot = sns.lineplot(\n",
    "    x=\"Transaction_Year\",\n",
    "    y=\"count\",\n",
    "    data=ev_count_df\n",
    ")\n",
    "ev_plot.set(xlabel =\"Year\", ylabel = \"Number of Transactions\", title ='Electric Vehicle Transactions in Washington State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9920f370-425b-4f9c-b27d-1aad11e31263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysis\n",
    "The above visualization displays the sharp increase in ownership.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6071627470413656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "WA_Electric_Vehicle",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
